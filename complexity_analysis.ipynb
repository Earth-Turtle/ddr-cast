{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "Defining constants and calculation functions",
   "id": "7479529c0982fe91"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "from typing import Generator\n",
    "\n",
    "directions = {\"U\": 0, \"R\": 1, \"D\": 2, \"L\": 3}\n",
    "\n",
    "def sequence_generation(string: str, max_len=8) -> Generator[str]:\n",
    "    yield string\n",
    "    if len(string) < max_len:\n",
    "        for valid_dir in {d for d in directions.keys() if directions[d] <= len(string)}:\n",
    "            yield from sequence_generation(string + valid_dir, max_len)\n",
    "\n",
    "def pattern_entropy(string: str):\n",
    "    entropy = 0\n",
    "    for char in directions.keys():\n",
    "        count = string.count(char)\n",
    "        if count == 0: continue\n",
    "        char_probability = float(count) / len(string)\n",
    "        entropy -= char_probability * math.log(char_probability, 2)\n",
    "    return entropy\n",
    "\n",
    "def lempel_ziv_stats(string):\n",
    "    encoding_dict = {}\n",
    "    for direction in directions.keys():\n",
    "        if direction in string:\n",
    "            encoding_dict[direction] = str(len(encoding_dict))\n",
    "\n",
    "    working_string = \"\"\n",
    "    output_codes = []\n",
    "    for c in string:\n",
    "        if working_string + c in encoding_dict:\n",
    "            working_string += c\n",
    "        else:\n",
    "            output_codes.append(encoding_dict[working_string])\n",
    "            encoding_dict[working_string + c] = str(len(encoding_dict))\n",
    "            working_string = c\n",
    "\n",
    "    output_codes.append(encoding_dict[working_string])\n",
    "    return len(output_codes), len(encoding_dict)\n"
   ],
   "id": "8827e53b5ccd3668",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's also define the \"strength\" of each challenge the player will have to go through. A rough analogue would be a level cap in a nuzlocke",
   "id": "1131071372ea1302"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "level_caps = [1.5 + (i * 0.1) + 0.3 * (i ** 1.8) for i in range(10)]\n",
    "level_caps"
   ],
   "id": "e0c70f858ec86be7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Start with all unique sequences up to some maximum length",
   "id": "6bd0f220a54d6ad7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 300)\n",
    "\n",
    "max_sequence_length = 12\n",
    "\n",
    "sequences = np.fromiter(sequence_generation(\"U\", max_sequence_length), \"U16\", count=-1)\n",
    "\n",
    "sequences_with_complexity = pd.DataFrame(sequences, columns=[\"Input pattern\"])\n",
    "just_sequences = sequences_with_complexity[\"Input pattern\"]"
   ],
   "id": "3357ba280672d9a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compute columns for length and entropy",
   "id": "879553b51b537711"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vectorized_length = np.vectorize(len)\n",
    "sequences_with_complexity[\"length\"] = vectorized_length(just_sequences)\n",
    "\n",
    "vectorized_entropy = np.vectorize(pattern_entropy)\n",
    "sequences_with_complexity[\"entropy\"] = vectorized_entropy(just_sequences)"
   ],
   "id": "7e61427dd5d1aa85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compute and concatenate columns for compression encoding stats",
   "id": "bd6b199d29372844"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vectorized_lzw = np.vectorize(lempel_ziv_stats)\n",
    "sequences_with_complexity = pd.concat(\n",
    "    [\n",
    "        sequences_with_complexity,\n",
    "        pd.DataFrame(np.array(vectorized_lzw(just_sequences)).T, columns=[\"compressed length\", \"encoding dict size\"]),\n",
    "    ], axis=1\n",
    ")\n",
    "\n",
    "sequences_with_complexity"
   ],
   "id": "95bb3926c2d27c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Scatter length vs entropy",
   "id": "46a3ed1d836a2a33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "points = sequences_with_complexity[[\"length\", \"entropy\"]].drop_duplicates()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(points[\"length\"], points[\"entropy\"])\n",
    "\n",
    "plt.show()"
   ],
   "id": "86fefeea79b84743",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Scatter length vs encoded length",
   "id": "fed6f8fa37bdfc21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "length_and_compressed_length = np.column_stack((sequences_with_complexity[\"length\"], sequences_with_complexity[\"compressed length\"]))\n",
    "lengths_T = length_and_compressed_length.T\n",
    "length_and_compressed_length = list(zip(lengths_T[0], lengths_T[1]))\n",
    "\n",
    "unique, counts = np.unique(length_and_compressed_length, axis=0, return_counts=True)\n",
    "unique = [(length, encoded_len) for length, encoded_len in unique]\n",
    "\n",
    "count_dict = dict(zip(unique, counts))\n",
    "\n",
    "def lookup(r):\n",
    "    return count_dict[(*r,)]\n",
    "\n",
    "big_dot_sizes = np.apply_along_axis(lookup, arr=sequences_with_complexity[[\"length\", \"compressed length\"]].to_numpy(), axis=1)"
   ],
   "id": "36244a5b5da0068",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "sequences_with_complexity[\"scaled dot sizes\"] = big_dot_sizes / (count_dict[max(count_dict, key=count_dict.get)] / 10) + 1\n",
    "sequences_with_complexity"
   ],
   "id": "bbd3b1949b4aecfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "points = sequences_with_complexity[[\"length\", \"compressed length\", \"scaled dot sizes\"]].drop_duplicates()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x=points[\"length\"],\n",
    "           y=points[\"compressed length\"],\n",
    "           s=points[\"scaled dot sizes\"],\n",
    "           )\n",
    "\n",
    "plt.show()"
   ],
   "id": "72e37a08f0162dec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And now for dict size",
   "id": "75dfe117f55624d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "fix, ax = plt.subplots()\n",
    "ax.scatter(x=sequences_with_complexity[\"length\"],\n",
    "           y=sequences_with_complexity[\"encoding dict size\"],\n",
    "           )\n",
    "\n",
    "plt.show()"
   ],
   "id": "704399b476e76185",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "See if there's large deviations between encoded length and encoding dict size",
   "id": "32545ce657c5ac33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fix, ax = plt.subplots()\n",
    "ax.scatter(sequences_with_complexity[\"compressed length\"], sequences_with_complexity[\"encoding dict size\"])\n",
    "\n",
    "plt.show()"
   ],
   "id": "d8f3acfa16931e61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Checking entropy vs encoding size",
   "id": "37bdb28967014703"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fix, ax = plt.subplots()\n",
    "points = sequences_with_complexity[[\"entropy\", \"compressed length\"]].drop_duplicates()\n",
    "ax.scatter(y=points[\"entropy\"], x=points[\"compressed length\"])\n",
    "\n",
    "plt.show()"
   ],
   "id": "42281e39bd2d8aa4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Trying out a formula, aiming for worst pattern to grow by 0.1 with length, and best pattern to grow quadratically.\n",
    "\n",
    "`RR` is 1.1, `RU` is 1.2.\n",
    "\n",
    "`RRR` is 1.2, `RUL` is 1.5\n",
    "\n",
    "`RRRR` is 1.3, `RULD` is 2\n",
    "\n",
    "Idea: linear \"base\" per length, then scale entropy based on the difference between length and compressed length"
   ],
   "id": "6e1ffb97ab4560bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# shorter name for conciseness\n",
    "seq = sequences_with_complexity\n",
    "\n",
    "seq[\"complexity\"] = (((seq[\"length\"] - 1) * 0.1 + 1)\n",
    "                     * (seq[\"entropy\"]\n",
    "                        * (seq[\"length\"]\n",
    "                           / (1 + seq[\"length\"] - seq[\"compressed length\"])\n",
    "                           )\n",
    "                        )\n",
    "                     )"
   ],
   "id": "43e464c0a85a1ebd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Graphing lengths vs complexity to see the spread",
   "id": "9fa5b9d565e9f15e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "points = seq[[\"Input pattern\", \"length\", \"complexity\", \"entropy\"]].drop_duplicates(subset=[\"length\", \"complexity\"])\n",
    "\n",
    "fix, ax = plt.subplots()\n",
    "ax.scatter(x=points[\"length\"],\n",
    "           y=points[\"complexity\"],\n",
    "           s=(points[\"entropy\"] * 10),)\n",
    "\n",
    "plt.show()"
   ],
   "id": "6316947652482c71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "That's an interesting gap, what's causing it?",
   "id": "521e6c848f55a895"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "twelve_length = points[points[\"length\"] == 12]\n",
    "twelve_length_below_gap = twelve_length[twelve_length[\"complexity\"] < 30]\n",
    "twelve_length_above_gap = twelve_length[twelve_length[\"complexity\"] >= 30]\n",
    "highest_below_gap = twelve_length_below_gap[twelve_length_below_gap[\"complexity\"] == max(twelve_length_below_gap[\"complexity\"])]\n",
    "lowest_above_gap = twelve_length_above_gap[twelve_length_above_gap[\"complexity\"] == min(twelve_length_above_gap[\"complexity\"])]\n",
    "print(highest_below_gap)\n",
    "print(lowest_above_gap)"
   ],
   "id": "f527e3bba8ad9d02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(seq[seq[\"Input pattern\"] == \"URRRUDULDDLL\"])\n",
    "print(seq[seq[\"Input pattern\"] == \"URRUUDRDDULU\"])"
   ],
   "id": "58cc6605b4d57be7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It looks like being able to compress the pattern just by one symbol is a big jump. Do the maximum complexity patterns have the shape we want?",
   "id": "b5589ecaf9251dc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(seq[seq[\"complexity\"] == max(seq[\"complexity\"])])",
   "id": "c525743c7a4cb720",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Too many repeated letters in each sequence. Let's add a metric to account for those",
   "id": "4b3031f8bddeb8aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def count_repeated_letters(s: str) -> int:\n",
    "    count = 0\n",
    "    for previous, current in zip(s, s[1:]):\n",
    "        if previous == current: count += 1\n",
    "    return count\n",
    "\n",
    "seq[\"repeated letters\"] = seq[\"Input pattern\"].apply(count_repeated_letters)\n",
    "\n",
    "seq[[\"Input pattern\", \"repeated letters\"]]"
   ],
   "id": "a0a79270e8a69f7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Setting the \"compressability multiplier\" to be equal to $$\\log_{10}({\\text{length} \\over 1 + \\text{length} - \\text{compressed length}})$$",
   "id": "5b63b17da8f035f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from math import log10\n",
    "\n",
    "seq[\"base length multiplier\"] = (seq[\"length\"] - 1) * 0.1 + 1\n",
    "seq[\"compressability multiplier\"] = (seq[\"length\"] / (1 + seq[\"length\"] - seq[\"compressed length\"])).apply(lambda x: log10(9 + (max(1, x))))\n",
    "seq[[\"Input pattern\", \"length\", \"compressability multiplier\"]].sort_values(\"compressability multiplier\")"
   ],
   "id": "3ca138be68fcb4a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# the lowest the bonus could be (asymptotically approaches). 1 is good, can be less than to reduce the chance any bonus is given for many repeats\n",
    "lower_bound = 0.97\n",
    "# How fast the maximum bonus scales based on length. This is an exponent, should probably keep between 1 and 2\n",
    "max_bonus_scaling = 1.3\n",
    "# At what length should the no-repeats bonus be 2\n",
    "length_for_double = 18\n",
    "# from 0 to 1, where 1 has the whole string as a repeated input, where half of the bonus should be received\n",
    "proportion_for_half_bonus = 0.2\n",
    "# steepness of the curve. Higher values make the curve steeper, and flatten the asymptotic approach on either side of the midpoint. Must be greater than 0\n",
    "steepness = 1.25\n",
    "# Additional parameter for affecting character of the curve. > 0\n",
    "skew = 1.5\n",
    "\n",
    "upper_bound = 1 + ((seq[\"length\"] / length_for_double) ** max_bonus_scaling)\n",
    "\n",
    "mid_shift_inside_ln = (((upper_bound - lower_bound) ** skew) / ((((upper_bound + lower_bound) / 2) - lower_bound) ** skew)) - 1\n",
    "\n",
    "mid_shift = seq[\"length\"] * proportion_for_half_bonus - (1.0 / steepness) * mid_shift_inside_ln.apply(np.log)\n",
    "\n",
    "exponent = steepness * (seq[\"repeated letters\"] - mid_shift)\n",
    "bottom_of_frac = (1 + exponent.apply(np.exp)) ** (1 / skew)\n",
    "seq[\"no repeats bonus\"] = (lower_bound + ((upper_bound - lower_bound) / bottom_of_frac)).apply(lambda x: max(1, x))\n",
    "seq[[\"length\", \"repeated letters\", \"compressability multiplier\", \"no repeats bonus\"]].sort_values(\"no repeats bonus\")"
   ],
   "id": "33a0d504721a125a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Initial values look alright, let's see the graph",
   "id": "822897643af98aec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "points = seq[[\"length\", \"no repeats bonus\"]].drop_duplicates()\n",
    "\n",
    "fix, ax = plt.subplots()\n",
    "ax.scatter(x=points[\"length\"],\n",
    "           y=points[\"no repeats bonus\"],)\n",
    "ax.grid('on')\n",
    "\n",
    "plt.show()"
   ],
   "id": "453768b39fe8540a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Very good spread. I like that it appears 12 with 5 repeated inputs gives the same bonus as 4 with 1. Let's calculate the new, improved complexity from this",
   "id": "7e5e05d3b5a9c4ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "compressability_factor = 0.5\n",
    "no_repeats_factor = 0.8\n",
    "entropy_factor = 0.5\n",
    "\n",
    "seq[\"improved complexity\"] = (seq[\"base length multiplier\"]\n",
    "                              * (((((seq[\"compressability multiplier\"] - 1) * compressability_factor) + 1)\n",
    "                              * (((seq[\"no repeats bonus\"] - 1) * no_repeats_factor) + 1)\n",
    "                              * (1 + (seq[\"entropy\"] * entropy_factor / 2))) ** 1.3)).round(2)\n",
    "\n"
   ],
   "id": "f859d7c8d6639280",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fix, ax = plt.subplots()\n",
    "points = pd.concat([seq[\"length\"], seq[\"improved complexity\"]], axis=1).drop_duplicates()\n",
    "\n",
    "ax.scatter(points[\"length\"], points[\"improved complexity\"])\n",
    "ax.grid('on')\n",
    "plt.show()"
   ],
   "id": "bf935f2d5f7fe39c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "That's looking good. Decent jump from full repeats to a little bit of change, to more quickly demonstrate that complexity is key. Can this handle exponential growth?",
   "id": "92d54394a5b490e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "downscale_factor = 1\n",
    "seq[\"downscaled complexity\"] = ((seq[\"improved complexity\"] - 1) / downscale_factor + 1) ** 1\n",
    "\n",
    "fix, ax = plt.subplots()\n",
    "points = pd.concat([seq[\"length\"], seq[\"downscaled complexity\"]], axis=1).drop_duplicates()\n",
    "\n",
    "\n",
    "ax.scatter(points[\"length\"], points[\"downscaled complexity\"])\n",
    "ax.grid('on')\n",
    "\n",
    "# lines showing the \"power level\" of each level cap. The idea is that each red line should provide the same amount of challenge, so the maximum complexity available needs to be provided accordingly\n",
    "# ax.hlines(list(filter(lambda x: x < max(seq[\"downscaled complexity\"]) + 1, level_caps)) , 0, 1, transform=ax.get_yaxis_transform(), colors=\"r\", linewidth=1)\n",
    "plt.show()"
   ],
   "id": "d1e74fdd1f779636",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Maaaaaaaaybe. Exponential grows very fast as the pattern gets large. Quadratic or cubic might be better, but we definitely want to capture the feeling of \"the 15th input is harder to remember than the 5th\". Still need to remember that challenges aren't going to be balanced around the max possible complexity for the max length available, but instead a linearly-increasing threshold. It should be possible to do a boss both around max-level 6-length and mid-level 9-length (which the graph here shows pretty nicely, hm)",
   "id": "b64801d9fa4c9fb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Otherwise, looking decent. How do patterns look?",
   "id": "7db63da9c1d5ba4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "percentiles = [0.0, 0.3, 0.6, 0.9, 1]\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 400)\n",
    "\n",
    "for i in seq[\"length\"].drop_duplicates():\n",
    "    at_len = seq[seq[\"length\"] == i][[\"Input pattern\", \"length\", \"entropy\", \"compressability multiplier\", \"downscaled complexity\"]]\n",
    "    quantiles = at_len[\"downscaled complexity\"].quantile(percentiles, \"nearest\")\n",
    "    print(at_len[at_len[\"downscaled complexity\"].isin(quantiles)].drop_duplicates([\"downscaled complexity\"]).sort_values(\"downscaled complexity\"))"
   ],
   "id": "1066c83d17ddb563",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
